FROM apache/airflow:2.7.0-python3.9

USER root

ARG SPARK_VERSION=3.2.3 \
HADOOP_VERSION=3.2
# Note: this is needed when you use Python 3.3 or greater
ENV SPARK_HOME=/opt/bitnami/spark  \
SPARK_OPTS="--driver-java-options=-Dlog4j.logLevel=info" \
SPARK_VERSION=${SPARK_VERSION} \
HADOOP_VERSION=${HADOOP_VERSION} \
PYTHONHASHSEED=1


RUN set -ex && \
    apt-get update && \
    apt-get install -yqq openssh-client wget openjdk-11-jdk libsndfile1 && \
    mkdir -p ${SPARK_HOME}  && \
    mkdir -p ${SPARK_HOME}/jars

# download spark-3.2.3-bin-hadoop3.2.tgz with commande :
# wget --no-verbose "https://archive.apache.org/dist/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz" &&
COPY jars_dependencies/spark-3.2.3-bin-hadoop3.2.tgz .
RUN tar -xf spark-3.2.3-bin-hadoop3.2.tgz -C /opt/bitnami/spark/  --strip-components=1 && \
    rm spark-3.2.3-bin-hadoop3.2.tgz

# Set JAVA_HOME
# change java-11-openjdk-amd64 to java-11-openjdk-arm64 if you are using arm arch processor
ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64
RUN export JAVA_HOME

# can't commit because of dir /jars-dependencies large files
# either download manually to /jar-dependencies or replace the copy with wget commands below:
# wget --no-verbose "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-s3/1.12.772/aws-java-sdk-s3-1.12.772.jar -P ${SPARK_HOME}/jars"
# wget --no-verbose "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.3/hadoop-aws-3.2.3.jar -P ${SPARK_HOME}/jars"
# wget --no-verbose "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-core/1.12.772/aws-java-sdk-core-1.12.772.jar -P ${SPARK_HOME}/jars"
# wget --no-verbose "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client/3.2.3/hadoop-client-3.2.3.jar -P ${SPARK_HOME}/jars"
# wget --no-verbose "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.772/aws-java-sdk-bundle-1.12.772.jar -P ${SPARK_HOME}/jars"

COPY jars_dependencies/aws-java-sdk-s3-1.12.772.jar ${SPARK_HOME}/jars
COPY jars_dependencies/hadoop-aws-3.2.3.jar ${SPARK_HOME}/jars
COPY jars_dependencies/aws-java-sdk-1.12.772.jar ${SPARK_HOME}/jars
COPY jars_dependencies/hadoop-client-3.2.3.jar ${SPARK_HOME}/jars
COPY jars_dependencies/aws-java-sdk-bundle-1.12.772.jar ${SPARK_HOME}/jars

USER airflow
RUN pip install --upgrade pip
COPY requirements.txt /
RUN  pip install --no-cache-dir -r /requirements.txt

# Specify the User that the actual main process will run as
#ARG AIRFLOW_UID="50000"
#USER ${AIRFLOW_UID}

USER airflow